{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e826906",
   "metadata": {
    "id": "0e826906"
   },
   "source": [
    "### The Smart Supplier: Optimizing Orders in a Fluctuating Market"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beffcce",
   "metadata": {
    "id": "2beffcce"
   },
   "source": [
    "Develop a reinforcement learning agent using dynamic programming to help a Smart Supplier decide which products to manufacture and sell each day to maximize profit. The agent must learn the optimal policy for choosing daily production quantities, considering its limited raw materials and the unpredictable daily demand and selling prices for different products.\n",
    "\n",
    "#### **Scenario**\n",
    " A small Smart Supplier manufactures two simple products: Product A and Product B. Each day, the supplier has a limited amount of raw material. The challenge is that the market demand and selling price for Product A and Product B change randomly each day, making some products more profitable than others at different times. The supplier needs to decide how much of each product to produce to maximize profit while managing their limited raw material.\n",
    "\n",
    "#### **Objective**\n",
    "The Smart Supplier's agent must learn the optimal policy π∗ using dynamic programming (Value Iteration or Policy Iteration) to decide how many units of Product A and Product B to produce each day to maximize the total profit over the fixed number of days, given the daily changing market conditions and limited raw material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wSIiywQLcad-",
   "metadata": {
    "id": "wSIiywQLcad-"
   },
   "source": [
    "**1. Custom Environment Creation (SmartSupplierEnv)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_fUZYRgym_Co",
   "metadata": {
    "id": "_fUZYRgym_Co"
   },
   "source": [
    "**Function to calculate Reward**\n",
    "\n",
    "Parameters:\n",
    "\n",
    "1.   env: object representing the environment\n",
    "2.   market: This argument represents the current market state.\n",
    "3.   a_units: This is the number of units of Product A produced in the current step.\n",
    "4.   b_units: This is the number of units of Product B produced in the current step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aGiLdci07rZ-",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1750780932322,
     "user": {
      "displayName": "PRADEEP SINGH BISHT",
      "userId": "08369692174587575624"
     },
     "user_tz": -330
    },
    "id": "aGiLdci07rZ-"
   },
   "outputs": [],
   "source": [
    "def calculateReward(env, market, a_units, b_units):\n",
    "    reward = (a_units * env.market_prices[market]['A'] +\n",
    "              b_units * env.market_prices[market]['B'])\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "syv79A55oOgQ",
   "metadata": {
    "id": "syv79A55oOgQ"
   },
   "source": [
    "**SmartSupplierEnv Class Explanation**\n",
    "\n",
    "This section of code defines the environment for the Smart Supplier problem class called SmartSupplierEnv. In reinforcement learning, the environment represents the world with which the agent interacts.\n",
    "\n",
    "The class has **reset** and **step** methods defined below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RhEIAXCm0vmD",
   "metadata": {
    "id": "RhEIAXCm0vmD"
   },
   "source": [
    "**Reset method**\n",
    "\n",
    "Parameter:\n",
    "\n",
    "1. self -  refers to the instance of the SmartSupplierEnv class.\n",
    "\n",
    "This code defines the reset method within the SmartSupplierEnv class. In the context of reinforcement learning, the reset method is crucial for starting a new episode (a complete run of the simulation, from the beginning to the end)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nEIbg6-i0z96",
   "metadata": {
    "id": "nEIbg6-i0z96"
   },
   "source": [
    "**Step method**\n",
    "\n",
    "Parameter:\n",
    "1. self - refers to the instance of the SmartSupplierEnv class.\n",
    "2. action_name - action name\n",
    "\n",
    "This code defines the step method within the SmartSupplierEnv class. In reinforcement learning, the step method is how the agent interacts with the environment. It takes an action as input and returns the resulting new state, the reward received, and whether the episode is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a4a087",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1750780932398,
     "user": {
      "displayName": "PRADEEP SINGH BISHT",
      "userId": "08369692174587575624"
     },
     "user_tz": -330
    },
    "id": "89a4a087"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from enum import Enum\n",
    "\n",
    "# Define market states and their product prices\n",
    "# Define product raw material costs\n",
    "\n",
    "class MarketState(Enum):\n",
    "    HIGH_DEMAND_A = 1\n",
    "    HIGH_DEMAND_B = 2\n",
    "\n",
    "class SmartSupplierEnv:\n",
    "    def __init__(self):\n",
    "        self.max_raw_material = 10\n",
    "        self.max_days = 5\n",
    "        self.product_a_rm_cost = 2\n",
    "        self.product_b_rm_cost = 1\n",
    "\n",
    "        # Define actions: (num_A, num_B, raw_material_cost_precalculated)\n",
    "        # Action ID mapping:\n",
    "        # 0: Produce_2A_0B\n",
    "        # 1: Produce_1A_2B\n",
    "        # 2: Produce_0A_5B\n",
    "        # 3: Produce_3A_0B\n",
    "        # 4: Do_Nothing\n",
    "        self.actions = {\n",
    "            'Produce_2A_0B': (2, 0),\n",
    "            'Produce_1A_2B': (1, 2),\n",
    "            'Produce_0A_5B': (0, 5),\n",
    "            'Produce_3A_0B': (3, 0),\n",
    "            'Do_Nothing': (0, 0)\n",
    "        }\n",
    "\n",
    "        # Structure: {Market_State_ID: {'A_price': X, 'B_price': Y}}\n",
    "        self.market_prices = {\n",
    "            MarketState.HIGH_DEMAND_A: {'A': 8, 'B': 2},\n",
    "            MarketState.HIGH_DEMAND_B: {'A': 3, 'B': 5}\n",
    "        }\n",
    "    def reset(self):\n",
    "      \"\"\"Reset the environment to initial state\"\"\"\n",
    "      self.current_day = 1\n",
    "      self.current_rm = self.max_raw_material\n",
    "      self.current_market = np.random.choice(list(MarketState))\n",
    "      return (self.current_day, self.current_rm, self.current_market.value)\n",
    "\n",
    "    def step(self, action_name):\n",
    "        \"\"\"Execute one step in the environment\"\"\"\n",
    "        if self.current_day > self.max_days:\n",
    "            raise ValueError(\"Episode has already ended\")\n",
    "\n",
    "        # Get production quantities for the action\n",
    "        a_units, b_units = self.actions[action_name]\n",
    "\n",
    "        # Calculate required raw material\n",
    "        required_rm = a_units * self.product_a_rm_cost + b_units * self.product_b_rm_cost\n",
    "\n",
    "        # Check if action is feasible\n",
    "        if required_rm <= self.current_rm:\n",
    "            # Calculate profit\n",
    "            profit = (a_units * self.market_prices[self.current_market]['A'] +\n",
    "                    b_units * self.market_prices[self.current_market]['B'])\n",
    "            self.current_rm -= required_rm\n",
    "        else:\n",
    "            # Action is not feasible, no production, no profit\n",
    "            profit = 0\n",
    "\n",
    "        # Prepare for next day\n",
    "        self.current_day += 1\n",
    "        self.current_rm = self.max_raw_material  # Daily reset\n",
    "        self.current_market = np.random.choice(list(MarketState))  # Random market state\n",
    "\n",
    "        # Check if episode is done\n",
    "        done = self.current_day > self.max_days\n",
    "\n",
    "        return (self.current_day, self.current_rm, self.current_market.value), profit, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VROKINrqr1eE",
   "metadata": {
    "id": "VROKINrqr1eE"
   },
   "source": [
    "**2. Dynamic Programming Implementation**\n",
    "\n",
    "**Value iteration method**\n",
    "\n",
    "The function takes three arguments:\n",
    "\n",
    "1.   env: This is an object representing the environment.\n",
    "2.   gamma: This is the discount factor, typically a value between 0 and 1. It determines the importance of future rewards compared to immediate rewards. A gamma of 1.0 means future rewards are just as important as immediate rewards.\n",
    "3.  theta: This is a small threshold value used to determine when the value iteration process has converged.\n",
    "\n",
    "The function iteratively updates the value of each possible state in the environment (V) until it converges to the optimal value function. It does this by repeatedly calculating the maximum expected future reward for each state, considering all possible actions and the transitions to subsequent states. Once the value function has converged, the optimal policy is derived by selecting the action that yields the highest value for each state. The code iterates backwards through the days, calculating the optimal values and actions. It also handles the random nature of the market state transitions and the daily reset of raw materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "027db857",
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1750780932464,
     "user": {
      "displayName": "PRADEEP SINGH BISHT",
      "userId": "08369692174587575624"
     },
     "user_tz": -330
    },
    "id": "027db857"
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=1.0, theta=1e-6):\n",
    "    \"\"\"Value Iteration algorithm to find optimal policy\"\"\"\n",
    "    # Initialize value function\n",
    "    V = np.zeros((env.max_days + 2, env.max_raw_material + 1, len(MarketState) + 1))\n",
    "\n",
    "    # Create action list for easy indexing\n",
    "    action_names = list(env.actions.keys())\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # Iterate through all possible states\n",
    "        for day in range(env.max_days, 0, -1):\n",
    "            for rm in range(env.max_raw_material + 1):\n",
    "                for market in MarketState:\n",
    "                    # Current state\n",
    "                    state = (day, rm, market.value)\n",
    "\n",
    "                    # Skip if day is beyond max_days\n",
    "                    if day > env.max_days:\n",
    "                        continue\n",
    "\n",
    "                    # Find the best action\n",
    "                    max_value = -float('inf')\n",
    "                    best_action = None\n",
    "\n",
    "                    for action_name in action_names:\n",
    "                        a_units, b_units = env.actions[action_name]\n",
    "                        required_rm = a_units * env.product_a_rm_cost + b_units * env.product_b_rm_cost\n",
    "\n",
    "                        # Calculate immediate reward\n",
    "                        if required_rm <= rm:\n",
    "                            reward = calculateReward(env, market, a_units, b_units)\n",
    "                            next_rm = env.max_raw_material  # Daily reset\n",
    "\n",
    "                            # Next market state is random with equal probability\n",
    "                            next_value = 0\n",
    "                            for next_market in MarketState:\n",
    "                                next_state = (day + 1, next_rm, next_market.value)\n",
    "                                next_value += 0.5 * V[next_state]\n",
    "                        else:\n",
    "                            reward = 0\n",
    "                            next_rm = env.max_raw_material  # Daily reset\n",
    "\n",
    "                            # Next market state is random with equal probability\n",
    "                            next_value = 0\n",
    "                            for next_market in MarketState:\n",
    "                                next_state = (day + 1, next_rm, next_market.value)\n",
    "                                next_value += 0.5 * V[next_state]\n",
    "\n",
    "                        # Calculate action value\n",
    "                        action_value = reward + gamma * next_value\n",
    "\n",
    "                        if action_value > max_value:\n",
    "                            max_value = action_value\n",
    "                            best_action = action_name\n",
    "\n",
    "                    # Update delta\n",
    "                    delta = max(delta, abs(max_value - V[state]))\n",
    "\n",
    "                    # Update value function\n",
    "                    V[state] = max_value\n",
    "\n",
    "        # Check for convergence\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Extract optimal policy\n",
    "    policy = {}\n",
    "    for day in range(1, env.max_days + 1):\n",
    "        for rm in range(env.max_raw_material + 1):\n",
    "            for market in MarketState:\n",
    "                state = (day, rm, market.value) #State structure\n",
    "\n",
    "                # Find best action for this state\n",
    "                best_action = None\n",
    "                best_value = -float('inf')\n",
    "\n",
    "                for action_name in action_names:\n",
    "                    a_units, b_units = env.actions[action_name]\n",
    "                    required_rm = a_units * env.product_a_rm_cost + b_units * env.product_b_rm_cost\n",
    "\n",
    "                    if required_rm <= rm:\n",
    "                        reward = calculateReward(env, market, a_units, b_units)\n",
    "                        next_rm = env.max_raw_material\n",
    "\n",
    "                        # Next market state is random with equal probability\n",
    "                        next_value = 0\n",
    "                        for next_market in MarketState:\n",
    "                            next_state = (day + 1, next_rm, next_market.value)\n",
    "                            next_value += 0.5 * V[next_state]\n",
    "                    else:\n",
    "                        reward = 0\n",
    "                        next_rm = env.max_raw_material\n",
    "\n",
    "                        # Next market state is random with equal probability\n",
    "                        next_value = 0\n",
    "                        for next_market in MarketState:\n",
    "                            next_state = (day + 1, next_rm, next_market.value)\n",
    "                            next_value += 0.5 * V[next_state]\n",
    "\n",
    "                    action_value = reward + gamma * next_value\n",
    "                    #select best action value\n",
    "                    if action_value > best_value:\n",
    "                        best_value = action_value\n",
    "                        best_action = action_name\n",
    "\n",
    "                policy[state] = best_action\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jNYhlWS6bOb5",
   "metadata": {
    "id": "jNYhlWS6bOb5"
   },
   "source": [
    "**Print the Policy table.**\n",
    "\n",
    "This function takes the learned optimal policy as input. The policy is a dictionary that maps each possible state to the best action to take in that state. A state is represented as a tuple containing the current day, the amount of raw material available, and the current market state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "smByI3x2iWS8",
   "metadata": {
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1750780932525,
     "user": {
      "displayName": "PRADEEP SINGH BISHT",
      "userId": "08369692174587575624"
     },
     "user_tz": -330
    },
    "id": "smByI3x2iWS8"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def print_policy_table(policy):\n",
    "  # Print the policy in a readable table format\n",
    "    print(\"\\n---------------------------------------------------------\")\n",
    "    display(HTML(\"<b><font size='+2'>Optimal Policy Table:</font></b>\"))\n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "    print(f\"{'Day':<5}{'RM':<5}{'Market':<10}{'Best Action':<15}\")\n",
    "    print(\"-\" * 35)\n",
    "\n",
    "    for day in range(1, 6):\n",
    "        for rm in [0,1,2,3,4,5,6,7,8,9,10]:\n",
    "            for market in MarketState:\n",
    "                state = (day, rm, market.value)\n",
    "                print(f\"{day:<5} {rm:<5} {market.name:<10} {policy[state]:<15}\")\n",
    "        print(\"-\" * 35 if day < 5 else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t0zqnb8o39pX",
   "metadata": {
    "id": "t0zqnb8o39pX"
   },
   "source": [
    "**Print Value table**\n",
    "\n",
    "The purpose of this function is to print a subset of the learned optimal values in a formatted table, making it easier to understand how the expected future profit changes based on the day, available raw material, and market condition.\n",
    "\n",
    "**Function will print values of day 1 and day 5 only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "u7F9V83c3684",
   "metadata": {
    "executionInfo": {
     "elapsed": 80,
     "status": "ok",
     "timestamp": 1750780932602,
     "user": {
      "displayName": "PRADEEP SINGH BISHT",
      "userId": "08369692174587575624"
     },
     "user_tz": -330
    },
    "id": "u7F9V83c3684"
   },
   "outputs": [],
   "source": [
    "def print_value_table(V):\n",
    "    #Print key values from the value function\n",
    "    print(\"\\n---------------------------------------------------------\")\n",
    "    display(HTML(\"<b><font size='+2'>Value Function Highlights:</font></b>\"))\n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "    print(f\"{'Day':<5}{'RM':<5}{'Market':<10}{'Value':<10}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Print values for day 1 with various RM and both markets\n",
    "    for rm in [0,1,2,3,4,5,6,7,8,9,10]:\n",
    "        for market in MarketState:\n",
    "            state = (1, rm, market.value)\n",
    "            print(f\"{1:<5}{rm:<5}{market.name:<10}${V[state]:<10.2f}\")\n",
    "\n",
    "    # Print values for day 5 with various RM and both markets\n",
    "    print(\"\\n\")\n",
    "    for rm in [0,1,2,3,4,5,6,7,8,9,10]:\n",
    "        for market in MarketState:\n",
    "            state = (5, rm, market.value)\n",
    "            print(f\"{5:<5}{rm:<5}{market.name:<10}${V[state]:<10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uXQwhIUBdmkT",
   "metadata": {
    "id": "uXQwhIUBdmkT"
   },
   "source": [
    "**3. Optimal Policy Analysis**\n",
    "\n",
    "Optimal Policy Analysis\n",
    "Let's analyze the learned optimal policy:\n",
    "\n",
    "1.   Market State Impact:\n",
    "  *   In Market State 1 (High Demand A), the policy favors producing more Product A when raw materials allow\n",
    "  *   In Market State 2 (High Demand B), the policy favors producing more Product B\n",
    "\n",
    "2.   Raw Material Impact:\n",
    "  *   With low remaining RM, the policy chooses actions that fully utilize available materials.\n",
    "  *   For intermediate RM, it selects combinations that maximize profit per RM (like 1A+2B in Market State 2)\n",
    "\n",
    "3.   Day Progression Impact:\n",
    "\n",
    "  *   On the last day (day 5), the policy becomes more aggressive since there's no future to consider.\n",
    "  *   Earlier days show more balanced choices considering future opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ufKGsl7q0Lm",
   "metadata": {
    "id": "7ufKGsl7q0Lm"
   },
   "source": [
    "**4. Performance Evaluation:**\n",
    "\n",
    "The function simulates the SmartSupplierEnv for a specified number of num_episodes = 1000.\n",
    "\n",
    "In each episode, it starts by resetting the environment. Then, it follows the provided policy, which dictates the action to take in each state. It calculates the reward for each action and accumulates the episode_profit. After an episode finishes (after 5 days), the episode_profit is added to the total_profit. Finally, it returns the average_profit across all episodes, providing an estimate of the policy's expected performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62490896",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1750780932603,
     "user": {
      "displayName": "PRADEEP SINGH BISHT",
      "userId": "08369692174587575624"
     },
     "user_tz": -330
    },
    "id": "62490896"
   },
   "outputs": [],
   "source": [
    "# simulate policy function - Simulates the learned policy over multiple runs to evaluate performance\n",
    "\n",
    "def evaluate_policy(env, policy, num_episodes=1000):\n",
    "    total_profit = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_profit = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode_profit += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_profit += episode_profit\n",
    "\n",
    "    average_profit = total_profit / num_episodes\n",
    "    return average_profit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aclhbM0FrSPD",
   "metadata": {
    "id": "aclhbM0FrSPD"
   },
   "source": [
    "**Main execution function.**\n",
    "\n",
    "This code block serves as the main entry point for running the Smart Supplier reinforcement learning simulation.\n",
    "\n",
    "We calculate the state-value function (V∗) for key states by calling method value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "oQaDAYTKktz0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3199
    },
    "executionInfo": {
     "elapsed": 368,
     "status": "ok",
     "timestamp": 1750780932972,
     "user": {
      "displayName": "PRADEEP SINGH BISHT",
      "userId": "08369692174587575624"
     },
     "user_tz": -330
    },
    "id": "oQaDAYTKktz0",
    "outputId": "1eb73a98-b913-41c5-f91e-ffd49ce4c2ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b><font size='+2'>Optimal Policy Table:</font></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "\n",
      "Day  RM   Market    Best Action    \n",
      "-----------------------------------\n",
      "1     0     HIGH_DEMAND_A Produce_2A_0B  \n",
      "1     0     HIGH_DEMAND_B Produce_2A_0B  \n",
      "1     1     HIGH_DEMAND_A Produce_2A_0B  \n",
      "1     1     HIGH_DEMAND_B Produce_2A_0B  \n",
      "1     2     HIGH_DEMAND_A Produce_2A_0B  \n",
      "1     2     HIGH_DEMAND_B Produce_2A_0B  \n",
      "1     3     HIGH_DEMAND_A Produce_2A_0B  \n",
      "1     3     HIGH_DEMAND_B Produce_2A_0B  \n",
      "1     4     HIGH_DEMAND_A Produce_2A_0B  \n",
      "1     4     HIGH_DEMAND_B Produce_1A_2B  \n",
      "1     5     HIGH_DEMAND_A Produce_2A_0B  \n",
      "1     5     HIGH_DEMAND_B Produce_0A_5B  \n",
      "1     6     HIGH_DEMAND_A Produce_3A_0B  \n",
      "1     6     HIGH_DEMAND_B Produce_0A_5B  \n",
      "1     7     HIGH_DEMAND_A Produce_3A_0B  \n",
      "1     7     HIGH_DEMAND_B Produce_0A_5B  \n",
      "1     8     HIGH_DEMAND_A Produce_3A_0B  \n",
      "1     8     HIGH_DEMAND_B Produce_0A_5B  \n",
      "1     9     HIGH_DEMAND_A Produce_3A_0B  \n",
      "1     9     HIGH_DEMAND_B Produce_0A_5B  \n",
      "1     10    HIGH_DEMAND_A Produce_3A_0B  \n",
      "1     10    HIGH_DEMAND_B Produce_0A_5B  \n",
      "-----------------------------------\n",
      "2     0     HIGH_DEMAND_A Produce_2A_0B  \n",
      "2     0     HIGH_DEMAND_B Produce_2A_0B  \n",
      "2     1     HIGH_DEMAND_A Produce_2A_0B  \n",
      "2     1     HIGH_DEMAND_B Produce_2A_0B  \n",
      "2     2     HIGH_DEMAND_A Produce_2A_0B  \n",
      "2     2     HIGH_DEMAND_B Produce_2A_0B  \n",
      "2     3     HIGH_DEMAND_A Produce_2A_0B  \n",
      "2     3     HIGH_DEMAND_B Produce_2A_0B  \n",
      "2     4     HIGH_DEMAND_A Produce_2A_0B  \n",
      "2     4     HIGH_DEMAND_B Produce_1A_2B  \n",
      "2     5     HIGH_DEMAND_A Produce_2A_0B  \n",
      "2     5     HIGH_DEMAND_B Produce_0A_5B  \n",
      "2     6     HIGH_DEMAND_A Produce_3A_0B  \n",
      "2     6     HIGH_DEMAND_B Produce_0A_5B  \n",
      "2     7     HIGH_DEMAND_A Produce_3A_0B  \n",
      "2     7     HIGH_DEMAND_B Produce_0A_5B  \n",
      "2     8     HIGH_DEMAND_A Produce_3A_0B  \n",
      "2     8     HIGH_DEMAND_B Produce_0A_5B  \n",
      "2     9     HIGH_DEMAND_A Produce_3A_0B  \n",
      "2     9     HIGH_DEMAND_B Produce_0A_5B  \n",
      "2     10    HIGH_DEMAND_A Produce_3A_0B  \n",
      "2     10    HIGH_DEMAND_B Produce_0A_5B  \n",
      "-----------------------------------\n",
      "3     0     HIGH_DEMAND_A Produce_2A_0B  \n",
      "3     0     HIGH_DEMAND_B Produce_2A_0B  \n",
      "3     1     HIGH_DEMAND_A Produce_2A_0B  \n",
      "3     1     HIGH_DEMAND_B Produce_2A_0B  \n",
      "3     2     HIGH_DEMAND_A Produce_2A_0B  \n",
      "3     2     HIGH_DEMAND_B Produce_2A_0B  \n",
      "3     3     HIGH_DEMAND_A Produce_2A_0B  \n",
      "3     3     HIGH_DEMAND_B Produce_2A_0B  \n",
      "3     4     HIGH_DEMAND_A Produce_2A_0B  \n",
      "3     4     HIGH_DEMAND_B Produce_1A_2B  \n",
      "3     5     HIGH_DEMAND_A Produce_2A_0B  \n",
      "3     5     HIGH_DEMAND_B Produce_0A_5B  \n",
      "3     6     HIGH_DEMAND_A Produce_3A_0B  \n",
      "3     6     HIGH_DEMAND_B Produce_0A_5B  \n",
      "3     7     HIGH_DEMAND_A Produce_3A_0B  \n",
      "3     7     HIGH_DEMAND_B Produce_0A_5B  \n",
      "3     8     HIGH_DEMAND_A Produce_3A_0B  \n",
      "3     8     HIGH_DEMAND_B Produce_0A_5B  \n",
      "3     9     HIGH_DEMAND_A Produce_3A_0B  \n",
      "3     9     HIGH_DEMAND_B Produce_0A_5B  \n",
      "3     10    HIGH_DEMAND_A Produce_3A_0B  \n",
      "3     10    HIGH_DEMAND_B Produce_0A_5B  \n",
      "-----------------------------------\n",
      "4     0     HIGH_DEMAND_A Produce_2A_0B  \n",
      "4     0     HIGH_DEMAND_B Produce_2A_0B  \n",
      "4     1     HIGH_DEMAND_A Produce_2A_0B  \n",
      "4     1     HIGH_DEMAND_B Produce_2A_0B  \n",
      "4     2     HIGH_DEMAND_A Produce_2A_0B  \n",
      "4     2     HIGH_DEMAND_B Produce_2A_0B  \n",
      "4     3     HIGH_DEMAND_A Produce_2A_0B  \n",
      "4     3     HIGH_DEMAND_B Produce_2A_0B  \n",
      "4     4     HIGH_DEMAND_A Produce_2A_0B  \n",
      "4     4     HIGH_DEMAND_B Produce_1A_2B  \n",
      "4     5     HIGH_DEMAND_A Produce_2A_0B  \n",
      "4     5     HIGH_DEMAND_B Produce_0A_5B  \n",
      "4     6     HIGH_DEMAND_A Produce_3A_0B  \n",
      "4     6     HIGH_DEMAND_B Produce_0A_5B  \n",
      "4     7     HIGH_DEMAND_A Produce_3A_0B  \n",
      "4     7     HIGH_DEMAND_B Produce_0A_5B  \n",
      "4     8     HIGH_DEMAND_A Produce_3A_0B  \n",
      "4     8     HIGH_DEMAND_B Produce_0A_5B  \n",
      "4     9     HIGH_DEMAND_A Produce_3A_0B  \n",
      "4     9     HIGH_DEMAND_B Produce_0A_5B  \n",
      "4     10    HIGH_DEMAND_A Produce_3A_0B  \n",
      "4     10    HIGH_DEMAND_B Produce_0A_5B  \n",
      "-----------------------------------\n",
      "5     0     HIGH_DEMAND_A Produce_2A_0B  \n",
      "5     0     HIGH_DEMAND_B Produce_2A_0B  \n",
      "5     1     HIGH_DEMAND_A Produce_2A_0B  \n",
      "5     1     HIGH_DEMAND_B Produce_2A_0B  \n",
      "5     2     HIGH_DEMAND_A Produce_2A_0B  \n",
      "5     2     HIGH_DEMAND_B Produce_2A_0B  \n",
      "5     3     HIGH_DEMAND_A Produce_2A_0B  \n",
      "5     3     HIGH_DEMAND_B Produce_2A_0B  \n",
      "5     4     HIGH_DEMAND_A Produce_2A_0B  \n",
      "5     4     HIGH_DEMAND_B Produce_1A_2B  \n",
      "5     5     HIGH_DEMAND_A Produce_2A_0B  \n",
      "5     5     HIGH_DEMAND_B Produce_0A_5B  \n",
      "5     6     HIGH_DEMAND_A Produce_3A_0B  \n",
      "5     6     HIGH_DEMAND_B Produce_0A_5B  \n",
      "5     7     HIGH_DEMAND_A Produce_3A_0B  \n",
      "5     7     HIGH_DEMAND_B Produce_0A_5B  \n",
      "5     8     HIGH_DEMAND_A Produce_3A_0B  \n",
      "5     8     HIGH_DEMAND_B Produce_0A_5B  \n",
      "5     9     HIGH_DEMAND_A Produce_3A_0B  \n",
      "5     9     HIGH_DEMAND_B Produce_0A_5B  \n",
      "5     10    HIGH_DEMAND_A Produce_3A_0B  \n",
      "5     10    HIGH_DEMAND_B Produce_0A_5B  \n",
      "\n",
      "\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b><font size='+2'>Value Function Highlights:</font></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "\n",
      "Day  RM   Market    Value     \n",
      "------------------------------\n",
      "1    0    HIGH_DEMAND_A$98.00     \n",
      "1    0    HIGH_DEMAND_B$98.00     \n",
      "1    1    HIGH_DEMAND_A$98.00     \n",
      "1    1    HIGH_DEMAND_B$98.00     \n",
      "1    2    HIGH_DEMAND_A$98.00     \n",
      "1    2    HIGH_DEMAND_B$98.00     \n",
      "1    3    HIGH_DEMAND_A$98.00     \n",
      "1    3    HIGH_DEMAND_B$98.00     \n",
      "1    4    HIGH_DEMAND_A$114.00    \n",
      "1    4    HIGH_DEMAND_B$111.00    \n",
      "1    5    HIGH_DEMAND_A$114.00    \n",
      "1    5    HIGH_DEMAND_B$123.00    \n",
      "1    6    HIGH_DEMAND_A$122.00    \n",
      "1    6    HIGH_DEMAND_B$123.00    \n",
      "1    7    HIGH_DEMAND_A$122.00    \n",
      "1    7    HIGH_DEMAND_B$123.00    \n",
      "1    8    HIGH_DEMAND_A$122.00    \n",
      "1    8    HIGH_DEMAND_B$123.00    \n",
      "1    9    HIGH_DEMAND_A$122.00    \n",
      "1    9    HIGH_DEMAND_B$123.00    \n",
      "1    10   HIGH_DEMAND_A$122.00    \n",
      "1    10   HIGH_DEMAND_B$123.00    \n",
      "\n",
      "\n",
      "5    0    HIGH_DEMAND_A$0.00      \n",
      "5    0    HIGH_DEMAND_B$0.00      \n",
      "5    1    HIGH_DEMAND_A$0.00      \n",
      "5    1    HIGH_DEMAND_B$0.00      \n",
      "5    2    HIGH_DEMAND_A$0.00      \n",
      "5    2    HIGH_DEMAND_B$0.00      \n",
      "5    3    HIGH_DEMAND_A$0.00      \n",
      "5    3    HIGH_DEMAND_B$0.00      \n",
      "5    4    HIGH_DEMAND_A$16.00     \n",
      "5    4    HIGH_DEMAND_B$13.00     \n",
      "5    5    HIGH_DEMAND_A$16.00     \n",
      "5    5    HIGH_DEMAND_B$25.00     \n",
      "5    6    HIGH_DEMAND_A$24.00     \n",
      "5    6    HIGH_DEMAND_B$25.00     \n",
      "5    7    HIGH_DEMAND_A$24.00     \n",
      "5    7    HIGH_DEMAND_B$25.00     \n",
      "5    8    HIGH_DEMAND_A$24.00     \n",
      "5    8    HIGH_DEMAND_B$25.00     \n",
      "5    9    HIGH_DEMAND_A$24.00     \n",
      "5    9    HIGH_DEMAND_B$25.00     \n",
      "5    10   HIGH_DEMAND_A$24.00     \n",
      "5    10   HIGH_DEMAND_B$25.00     \n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b><font size='+2'>Average total profit over 5 days: $122.53</font></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "/n------Program completed successfully-----\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "from IPython.display import display, HTML\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create environment\n",
    "    env = SmartSupplierEnv()\n",
    "\n",
    "    # Run value iteration\n",
    "   # Update the state values and get Policy and state table.\n",
    "    V, policy = value_iteration(env)\n",
    "\n",
    "    # Print policy and value tables\n",
    "    print_policy_table(policy)\n",
    "    print_value_table(V)\n",
    "\n",
    "    # Evaluate policy\n",
    "    print(f\"-----------------------------------------------------------------------------------------\")\n",
    "    print(f\"-----------------------------------------------------------------------------------------\")\n",
    "    avg_profit = evaluate_policy(env, policy, 1000)\n",
    "    display(HTML(f\"<b><font size='+2'>Average total profit over 5 days: ${avg_profit:.2f}</font></b>\"))\n",
    "    print(f\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "    print(f\"/n------Program completed successfully-----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3305647e",
   "metadata": {
    "id": "3305647e"
   },
   "source": [
    "**5. Impact of Dynamics Analysis**\n",
    "\n",
    "### Fixed Market State 1 vs. Fluctuating Market:\n",
    "1. Fixed Market State 1:\n",
    "\n",
    "\n",
    "*   Policy would always favor Product A production\n",
    "\n",
    "*   Optimal strategy would be to produce maximum A (3A when possible)\n",
    "\n",
    "*   Less consideration for Product B\n",
    "\n",
    "2.  Fluctuating Market:\n",
    "\n",
    "*   Policy must balance between both products\n",
    "\n",
    "*   More conservative in early days to preserve flexibility\n",
    "\n",
    "*   Values Product B more highly when Market State 2 is possible\n",
    "\n",
    "*   Overall strategy is more adaptive and robust\n",
    "\n",
    "The dynamic environment requires the agent to develop a more flexible strategy that can capitalize on whichever market state emerges each day, rather than specializing in just one product. This leads to a more balanced production approach that achieves good performance across both market conditions.\n",
    "\n",
    "The key insight is that in a fluctuating market, the optimal policy values flexibility and adapts production to current conditions, while in a fixed market it can specialize completely in the most profitable product.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
